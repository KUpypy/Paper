{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION\n",
    "\n",
    "> https://arxiv.org/pdf/1409.1556.pdf\n",
    "\n",
    "- __발표자 : 정지원__\n",
    "- __발표일 : 2017. 8. 7(월)__\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABSTRACT\n",
    "\n",
    "이 연구에서는 CNN 이미지 인식에서, 깊이가 정확도에 미치는 영향을 조사한다. 3x3 필터를 사용하여 깊이가 증가할수록 어떤 식으로 변하는지 살펴본다. 16-19개의 layer를 사용하며, 2014 ImageNet Challenge에서 1,2위를 차지했다. 앞으로 컴퓨터 비전에서도 쓰이기 위해 좋은 성능을 내는 두 가지 ConvNet을 공개한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## INTRODUCTION\n",
    "\n",
    "CNN은 GPU 분산 시스템을 통해 최근 많은 업적을 냈다. ILSVRC는 대규모 이미지 분류 시스템의 큰 발전을 가져왔다.\n",
    "\n",
    "ConvNets이 컴퓨터 비전 분야의 필수가 되면서, 많은 시도가 있었다. filter 사이즈와 stride를 줄여보거나, 여러 scale을 통해 전체 이미지를 학습시키는 등의 방법을 사용했다. 또한 주로 논문에서는 깊이에 대한 얘기를 한다. 다른 parameter들을 고정한 후, 깊이를(conv layer) 늘려가며 결과를 봤다. 3x3 filter를 사용하기 때문에 가능한 일이었다.\n",
    "\n",
    "결과적으로 ILSVRC 분류 및 Localisation 작업에 대해 좋은 성능을 냈고, 다른 이미지 인식에서도 쓰일 좋은 아키텍처가 탄생했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONVNET CONFIGURATIONS\n",
    "\n",
    "Depth를 관찰하기 위해 나머지는 동등한 layer로 구성했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARCHITECTURE\n",
    "\n",
    "![Image](figures/1.png)\n",
    "\n",
    "![Image](figures/2.png)\n",
    "\n",
    "ConvNets에 입력되는 사이즈는 224x224 RGB.\n",
    "\n",
    "오직 RGB Mean 값을 빼는 전처리를 한다.(AlexNet과 동일)\n",
    "\n",
    "3x3 filter를 사용한다.\n",
    "\n",
    "이 중 한번은 1x1 filter를 사용하는데, 입력 채널의 선형 변환이라 보면 된다.\n",
    "\n",
    "conv stride는 1로 고정한다.\n",
    "\n",
    "이러한 spatial padding은 convolution 이후 spatial resolution을 보존하기 위함이다.\n",
    "\n",
    "Spatial pooling은 5개의 max-pooling layer에서 진행된다.\n",
    "\n",
    "Max-pooling은 2x2 windows with stride 2\n",
    "\n",
    "여러 conv-layers 후에 3개의 fully-connected layers를 거치게 된다.\n",
    "\n",
    "4096-1000-1000이며, 마지막은 soft-max layer다.\n",
    "\n",
    "모든 Hidden layer에서 ReLU를 사용하며, LRN는 사용하지 않는다.(성능은 향상되지 않으면서 메모리/계산량만 늘어난다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONFIGURATIONS\n",
    "\n",
    "그림을 보면, A-E까지 깊이의 차이가 난다.(11-19)\n",
    "\n",
    "Conv-layer의 너비는 64로 시작하여 2개의 max-pooling을 거쳐 512가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DISCUSSION\n",
    "\n",
    "이 논문에서의 ConvNet은 기존의 Net과 많이 다르다.\n",
    "\n",
    "First Conv layers에서 11x11 with stride 4 / 7x7 with stride 2 등과 같이 큰 receptive field를 사용하기 보다는,\n",
    "\n",
    "아주 작은 3x3 receptive field with stride 1를 사용했다.\n",
    "\n",
    "2개의 stacked two 3x3 layers는 1개의 5x5 layer 효과를 얻는다.\n",
    "\n",
    "3개는 7x7의 효과를 얻는다.\n",
    "\n",
    "하나의 7x7 filter를 쓴 layer를 사용하는 것보다 세 개의 3x3 filter layers를 사용함으로써 이점이 무엇인가?\n",
    "\n",
    "우선, 1개가 아닌 3개의 ReLU를 통합하여 쓰므로, Decision function을 더욱 discriminative하게 만든다.\n",
    "\n",
    "input과 output 모두 3개의 3x3 layers와 C개의 channels을 사용한다고 가정하면,\n",
    "\n",
    "$3(3^{2}C^{2}) = 27C^{2}$의 weights가 필요하다.\n",
    "\n",
    "하지만, 7x7을 사용한다면, $7^{2}C^{2} = 49C^{2}$가 필요하다.\n",
    "\n",
    "81%가 더 필요하게 된다.(49/27~1.81)\n",
    "\n",
    "따라서 3x3로 3개를 쓰는 것이 좋다.\n",
    "\n",
    "1x1 conv layer를 사용하는 것은 conv layer의 receptive field에 영향을 주지 않고, decision function에 비선형성을 증가시키는 방법이다.\n",
    "\n",
    "1x1 convolution은 원래 같은 공간으로의 projection이기 때문에 선형적으로 보이지만, ReLU(activation func)에 의해 비선형성을 갖게 된다.\n",
    "\n",
    "이러한 작은 필터는 이미 많이 사용 됐었지만, network가 깊지 않아서 평가하지 않았었다.\n",
    "\n",
    "Goodfellow는 표지판 번호 인식에서 11 depth를 사용하면서 깊이가 깊어질수록 성능이 향상되는 것을 발견했다.\n",
    "\n",
    "GoogLeNet은 ILSVRC-2014에서 1위를 차지했고 우리와는 독립적으로 연구가 됐지만, 작은 filter를 사용한 깊은 망을 만든 점은 유사하다.\n",
    "\n",
    "차이점으로는 GoogLeNet의 형상은 비교적 매우 복잡하며, 계산량을 줄이기 위해 feature map의 공간 해상도를 first layer부터 매우 줄여나간다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFICATION FRAMEWORK\n",
    "\n",
    "ConvNet의 분류에 대해 자세히 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING\n",
    "\n",
    "__Multinomial Logistic regression objective__\n",
    "\n",
    "__Mini-batch Gradient Descent with momentum__\n",
    "\n",
    "batch size는 256, momentum은 0.9\n",
    "\n",
    "weight decay($L_2$ 정규화 $5*10^{-4}$)\n",
    "\n",
    "dropout: two fully-connected layer에 대해 0.5\n",
    "\n",
    "$\\eta = 10^{-2}$(10번의 validation set의 accuracy 개선이 없는 경우 10으로 나눔)\n",
    "\n",
    "학습동안 3번의 감소가 있었으며, 370K iteration(74 epochs)\n",
    "\n",
    "작은 filter로 깊게 짜여진 것과, 특정 층을 pre-initialization한 이유 때문에 parameter가 많음에도 불구하고 비교적 빠르게 수렴한 것으로 보여진다.\n",
    "\n",
    "Gradient가 불안정해지는 일이 생길 수 있으므로, 초기화는 매우 중요하다.\n",
    "\n",
    "이런 문제를 피하기 위하여 무작위 초기화 학습을 통해 얕은 층을 학습시켰다.\n",
    "\n",
    "그 후에 더 깊은 아키텍처를 학습시킬 때, 처음 4개의 conv-layer와 마지막 3개의 fully-connected layer를 앞선 아키텍처 layer로 초기화시켰다.\n",
    "\n",
    "사전 초기화된 layer의 학습시킬 때는 learning rate를 줄이지 않았다.\n",
    "\n",
    "Random normal, mean=0, std=0.01, bias=0\n",
    "\n",
    "논문 제출후에나 알게 된 사실은, Glorot & Bengio의 논문에서 나온 초기화 방법을 사용하면, pretrain 없이도 weight 초기화가 가능하다는 것이다.\n",
    "\n",
    "224x224를 얻기 위해서, 이미지를 무작위로 잘라냈다. horizontal flipping 및 RGB color shift(AlexNet과 동일)을 사용하여 Augmentation\n",
    "\n",
    "__Training image size__\n",
    "\n",
    "학습 이미지를 스케일 S를 정하는데 두 가지 접근방법을 사용했다.\n",
    "\n",
    "- #### single-scale training을 위해 S를 고정\n",
    "\n",
    "> 실험에서, S를 256과 384로 고정하여 학습된 모델을 평가했다.\n",
    "\n",
    "> S=256를 먼저 학습한 후, 이 weights를 가지고 S=384 network에 사용했으며, 학습률은 조금 더 작은 값인 0.001을 사용했다.\n",
    "\n",
    "- #### multi-scale training을 위해 S를 $[S_{min},S_{max}]$(여기서는 256,512) 사이의 random한 값으로 지정\n",
    "\n",
    "> 실제 이미지 크기가 모두 다르기 때문에, 이 방법은 학습에 매우 도움이 된다.\n",
    "\n",
    "> 먼저 S=384로 학습된 모델에서, S를 random하게 바꿔가며 fine-tune 한다.\n",
    "\n",
    "> 이러한 방법을 __Scale Jittering__이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING\n",
    "\n",
    "테스트 단계에서, 먼저 가장 작은 이미지 side인 Q로 rescaled 된다.\n",
    "\n",
    "Q는 scale인 S와 같을 필요는 없다.\n",
    "\n",
    "그 후, Network는 조밀하게 rescaled된 test image에 적용된다.\n",
    "\n",
    "Fully-connected layers는 우선 conv-layer로 전환된다.(First FC layer는 7x7 conv layer로, Second FC layer는 1x1 conv layer로)\n",
    "\n",
    "Fully-conv net의 결과는 (uncropped)모든 이미지에 적용된다.\n",
    "\n",
    "결과적으로 input image size에 따라 가변적인 공간 해상도를 갖는 Class의 수와 같은 Channel 수를 갖는 Class score map을 얻게 된다.\n",
    "\n",
    "마지막으로, 각 이미지에 대한 고정 사이즈의 Class score vector를 얻기 위해, Class score map을 평균시킨다.(sum-pooled)\n",
    "\n",
    "또한 horizontal filpping를 하여, test set을 보강한다.\n",
    "\n",
    "이렇게 구해진 posterior들(original & flipped)을 평균내어 최종 score로 사용한다.\n",
    "\n",
    "Fully-conv network는 모든 이미지에 적용되므로, multiple crop을 할 필요가 없다.\n",
    "\n",
    "동시에, 많은 crops을 시도할 수 있으므로 일반적인 방법에 비해 finer sampling이 가능하며 이를 통해 정확도를 올릴 수 있었다.\n",
    "\n",
    "또한 multi-crop evaluation은 다른 convolution boundary condition 때문에 dense evaluation을 보완하는 역할을 한다.\n",
    "\n",
    "ConvNet에 crop을 적용할 때는 convolved feature map은 zero-padding하는 반면에,\n",
    "\n",
    "dense evaluation의 경우 이미지의 이웃하는 부분에 대한 패딩이 자연스레 일어난다.(due to both the convolution and spatial pooling)\n",
    "\n",
    "이는 전체 네트워크의 receptive field를 실질적으로 증가시키므로, 더 많은 context가 포착된다.\n",
    "\n",
    "실제로 이 같은 방법을 사용하여 계산량이 많아지는 것이 정확도를 얼마나 얻게 해주는지는 모르지만,\n",
    "\n",
    "150 crops over 3 scales 과 144 crops over 4 scales은 차이를 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION DETAILS\n",
    "\n",
    "Caffe\n",
    "\n",
    "GPU 병렬 처리, 각 이미지 batch를 GPU로 분할시킨 후, Gradient를 구할 때 평균시킨다.\n",
    "\n",
    "여러 layer에 대해 병렬 처리를 사용하는 정교한 방법이 최근 제안됐지만, 개념적으로 훨씬 간단한 방법이 이미 multi-GPU에서 3.75배 빠른 속도를 갖는다.\n",
    "\n",
    "Nvidia titan black로 2~3주가 걸렸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLASSIFICATION EXPERIMENTS\n",
    "\n",
    "1,000개의 Class\n",
    "\n",
    "1.3M training image\n",
    "\n",
    "50k validation image\n",
    "\n",
    "100k test image(labeled)\n",
    "\n",
    "top-1 / top-5 error rate\n",
    "\n",
    "\"VGG\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SINGLE SCALE EVALUATION\n",
    "\n",
    "학습 결과는 다음과 같다.\n",
    "\n",
    "![Image](figures/3.png)\n",
    "\n",
    "LRN은 효과가 없어서 쓰지 않는다.\n",
    "\n",
    "depth가 깊어질수록 error가 줄어드는 것을 볼 수 있다.\n",
    "\n",
    "11(A) to 19(E)\n",
    "\n",
    "같은 깊이일 때, three 1x1 conv layer를 가진 C가 3x3 conv layer를 쓴 D보다 좋지 않음을 볼 수 있다.\n",
    "\n",
    "이것은 추가적인 non-linearity가 도움이 됨을 보여준다.(C가 B보다 좋다는 면에서)\n",
    "\n",
    "Error rate는 depth 19에서 더 좋아지지 않지만, 더 큰 data set에서는 더 깊을수록 좋을 것이다.\n",
    "\n",
    "5개의 5x5 conv layer를 사용한 B와, 그것들을 3x3 conv layer로 교체한 것을 비교했는데, 7%가 좋아졌다.\n",
    "\n",
    "이를 통해 shallow net에서, 작은 필터가 더 좋은 성능을 낸다는 것을 알 수 있다.\n",
    "\n",
    "마지막으로 training에서 scale jittering은 fixed smallest side(S=256 or S=384)보다 더 좋은 결과를 낸다.(test에서 single을 쓰더라도)\n",
    "\n",
    "이는 scale jittering이 도움이 됨을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTI-SCALE EVALUATION\n",
    "\n",
    "![Image](figures/4.png)\n",
    "\n",
    "several rescaled versions of test image.\n",
    "\n",
    "__scale jittering__\n",
    "\n",
    "class posterior에 대한 averaging\n",
    "\n",
    "Q = {S - 32, S, S + 32}\n",
    "\n",
    "Q = {$S_{min} , 0.5(S_{min}+S_{max}), S_{max}$}\n",
    "\n",
    "train에서의 S는 고정된 것보다 scale jittering된 것이 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MULTI-CROP EVALUATION\n",
    "\n",
    "![Image](figures/5.png)\n",
    "\n",
    "- dense\n",
    "\n",
    "- multi-crop evaluation\n",
    "\n",
    "- averaging soft max outputs.(dense + multi-crop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CONVNET FUSION\n",
    "\n",
    "![Image](figures/6.png)\n",
    "\n",
    "Ensemble,\n",
    "\n",
    "soft-max class posteriors를 평균내어 결합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COMPARISON WITH THE STATE OF THE ART\n",
    "\n",
    "![Image](figures/7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION\n",
    "\n",
    "깊이가 중요하다.\n",
    "\n",
    "기존의 Conv layer를 깊히 쌓아서 좋은 결과를 냈다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. LOCALISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1 LOCALISATION CONVNET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2 LOCALISATION EXPERIMENTS\n",
    "\n",
    "![Image](figures/9.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. GENERALISATION OF VERY DEEP FEATURES\n",
    "\n",
    "\n",
    "![Image](figures/11.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
