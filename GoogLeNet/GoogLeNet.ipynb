{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going deeper with convolutions\n",
    "\n",
    "> https://arxiv.org/pdf/1409.4842.pdf\n",
    "\n",
    "- __발표자 : 정지원__\n",
    "- __발표일 : 2017. 8. 10(목)__\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "ILSVRC14에서 Inception이라는 이름의 깊은 CNN 아키텍처를 제안한다. 주된 특징은 Network 내부에서 컴퓨팅 리소스를 효율적으로 사용한다는 것이다. 이로 인해 계산량을 아끼면서 depth와 width를 늘릴 수 있었다. 최적화를 위해서, 아키텍처 결정을 Hebbian principle과 multi-scale processing을 기반으로 하였다. 22개의 layers로 깊게 싸여진 네트워크는 분류 및 디텍션 영역에서 평가된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "지난 3년간, 이미지 인식 분야는 급속히 발달했다. 하드웨어나 데이터셋 혹은 더 큰 모델을 만들어서가 아니라, 새로운 아이디어와 알고리즘 및 개선된 네트워크 아키텍처 덕분이다. 실제 이 논문의 GoogLeNet은 2년 전, 아키텍처보다 12배 적은 매개변수를 사용하면서 더 정확한 결과를 낸다. Object detection에서 가장 큰 발전은 더 깊거나 큰 모델을 쓰는 것이 아니라, R-CNN과 같은 알고리즘과 고전적인 컴퓨터 비전의 시너지였다.\n",
    "\n",
    "또 다른 중요한 요소는 모바일이나 임베디드 컴퓨팅의 발달로 인해 알고리즘의 효율, 특히나 메모리 사용이나 전력 사용이 중요해졌다. 이 논문에서는 Accuracy에만 집중하는 것이 아니라 이러한 요소들에 대해서도 얘기하고자 한다. 대부분의 실험에서 모델은 1.5 billion의 연산을 하도록 유지했으며, 따라서 실제로 사용해볼만한 결과를 냈다.\n",
    "\n",
    "컴퓨터 비전을 위한 효율적인 Deep NN 아키텍처에 집중할 것이다. \"We need to go deeper\"에서 Inception이라는 이름을 따왔다. 이 아키텍처는 ILSVRC 2014 분류 및 디텍션 문제에 대해 최고의 성능을 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Related Work\n",
    "\n",
    "LeNet-5에서 시작하여, CNN는 standard한 구조를 가졌다. 보통 conv-layer가 쌓인 후에 하나 혹은 그 이상의 FC layer가 온다. 혹은 오버피팅을 막기 위해 drop-out을 사용하는 구조이다.\n",
    "\n",
    "Max-pooling layer가 공간 정보의 손실을 낼 수 있다는 우려에도 불구하고, AlexNet과 같은 CNN 아키텍처는 Localization/Object detection/Human pose estimation 등에 잘 적용됐다. 시각 피질로부터 영감을 얻은 뉴로사이언스 모델은 여러 다른 Gabor filters 사이즈를 적용했다. 그러나 고정된 2-layer 모델과는 다르게 Inception 모델에서는 모든 필터가 학습된다. 게다가 Inception layers는 반복적으로 수행되며, 22-layer를 만들게 된다.\n",
    "\n",
    "NIN(Network-in-Network)는 1x1 Conv layer다. 이 방식을 이용하면 차원을 축소시킴으로써 연산량의 병목을 줄이게 된다. 이로인해 depth를 늘릴 수 있으며, 성능의 저하 없이 네트워크의 너비도 늘릴 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation and High Level Considerations\n",
    "\n",
    "![Image](figures/1.png)\n",
    "\n",
    "DNN의 성능을 향상시키는 방법은 size를 키우는 것이다. 깊이/너비\n",
    "\n",
    "하지만 무작정 늘리면 단점이 있다.\n",
    "\n",
    "크기가 커지면 parameter 수가 많아지므로, overfitting이 발생한다.\n",
    "\n",
    "특히나 각 label별로 학습 데이터가 제한적인 경우 더 그렇다.\n",
    "\n",
    "또 다른 단점은, 계산량이 매우 증가한다는 것이다.\n",
    "\n",
    "예를 들어, conv-layer에서 filter 수가 증가하면 계산이 quadratic 하게 증가한다.\n",
    "\n",
    "추가된 capacity가 비효율적으로 사용되는 경우, 많은 연산이 낭비된다.\n",
    "\n",
    "실제 연산에 사용될 budget은 제한돼 있으므로, 효율적인 컴퓨팅 리소스가 필요하다.\n",
    "\n",
    "이러한 두 문제를 해결하기 위해서는 완전히 연결된 아키텍처를 sparsely 연결된 아키텍처로 변경해보는 것이다.\n",
    "\n",
    "layer의 activations과 outputs간의 연관성을 분석하는 등의 확률 분포를 살펴보는 방식이 있을 것이다.\n",
    "\n",
    "비록 엄격하게 수식적으로 증명되려면 매우 조건이 엄격하지만, 잘 알려진 Hebbian principle을 통해 덜 엄격한 조건에서도 적용해볼 수 있다.\n",
    "\n",
    "단점이라면, 현재 컴퓨팅은 비균일한 sparse 구조에 대해 매우 비효율적이다.\n",
    "\n",
    "ConvNet은 전통적으로 feature 차원에서 랜덤하고 sparse한 테이블을 사용했으나, 병렬처리를 최적화 하여 학습을 더 잘하기 위해서 full connection 방식으로 바뀌는 추세다.\n",
    "\n",
    "구조의 균일성과 많은 filter수는 더욱 효율적인 연산을 가능하게한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectural Details\n",
    "\n",
    "\n",
    "최적의 local construction을 발견하고 그것을 Spatially Repeat\n",
    "\n",
    "마지막 층의 correlation 통계를 분석하고, 높은 상관 관계가 있는 단위 그룹으로 클러스터링 하자.\n",
    "\n",
    "이러한 클러스터는 다음 계층의 단위를 형성하고 이전 계층의 단위에 연결된다.\n",
    "\n",
    "![Image](figures/2.png)\n",
    "\n",
    "![Image](figures/inception.png)\n",
    "\n",
    "많은 수의 필터가 있으면 3x3 / 5x5를 사용할 때 계산량이 많아진다.\n",
    "\n",
    "너무 많은 정보가 있을수도 있으니 압축하자.\n",
    "\n",
    "1x1 conv를 통해 압축한다. 그 후에 3x3/5x5를 사용하자.\n",
    "\n",
    "다양한 스케일로 시각 정보가 처리되며, 마지막에 정렬이 된다.\n",
    "\n",
    "비교적 연산량이 줄어들며, 결과적으로 깊게 쌓을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogLeNet\n",
    "\n",
    "\n",
    "![Image](figures/3.png)\n",
    "\n",
    "Inception 모듈을 포함한 모든 Conv-layer는 ReLU를 사용\n",
    "\n",
    "네트워크 Receptive field size는 224 × 224이며, 평균값을 갖는 RGB 색상 채널을 사용\n",
    "\n",
    "네트워크는 계산 효율성과 실용성을 염두에두고 설계되었으므로 메모리가 부족한 경우가 있을 수 있다.\n",
    "\n",
    "네트워크는 parameter가 있는 layer만 22개(또는 풀링을 포함하면 27개의 layer)\n",
    "\n",
    "네트워크 구축에 사용 된 전체 계층 수(독립 빌드 블록)는 약 100입니다.\n",
    "\n",
    "FC layer-> averaging pooling을 이용하여 0.6% 향상, 하지만 drop-out은 여전히 사용\n",
    "\n",
    "![Image](figures/4_.png)\n",
    "\n",
    "Auxiliary Classifier를 사용.\n",
    "\n",
    "Gradient를 전파하면, Vanishing 문제를 해결할 수 있다.\n",
    "\n",
    "Auxiliary Classifier를 포함하여 측면에 있는 추가 네트워크의 정확한 구조는 다음과 같다.\n",
    "\n",
    "- filter=5×5, stride=3인 Average pooling layer, (4a)에 대해서는 4×4×512이고, (4d)에 대해서는 4×4×528이다.\n",
    "- 차원 축소 및 선형 선형 활성화를위한 128 개의 필터가있는 1 × 1 컨볼 루션입니다.\n",
    "- FC with 1024, ReLU\n",
    "- 70% drop-out\n",
    "- Linear layer with Softmax loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Methodology\n",
    "\n",
    "병렬성을 최대한 끌어올리기 위해 DistBelief 분산 시스템을 사용\n",
    "\n",
    "Momentum=0.9, fixed learning rate schedule(decresing 4% every 8 epochs)\n",
    "\n",
    "최종 모델은 inference 단계에서 Polyak averaging을 사용.\n",
    "\n",
    "Sampling 방법은 competition동안 계속해서 변해왔으며, 이미 여러 하이퍼파라미터들을 변경해가며 수렴된 모델들에 대해서 하나의 가장 효율적인 방법을 제시하는 것은 어려웠다.\n",
    "\n",
    "몇 모델은 작은 crop에 대해 학습하였고, 다른 것들은 큰 crop에 대해 학습\n",
    "\n",
    "측광 왜곡이 어느 정도 overfitting을 방지하는데 유용함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ILSVRC 2014 Classification Challenge Setup and Results\n",
    "\n",
    "![Image](figures/5.png)\n",
    "![Image](figures/6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ILSVRC 2014 Detection Challenge Setup and Results\n",
    "\n",
    "![Image](figures/7.png)\n",
    "![Image](figures/8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Optimal sparse structure를 근사화 하는 것이 컴퓨터 비전에서 NN을 개선하기 위한 좋은 방법이라는 것을 증명했다.\n",
    "\n",
    "이 방법의 장점은 적은 폭의 네트워크와 비교하여 계산량이 약간 증가할 때 성능이 많이 좋아진다는 점이다.\n",
    "\n",
    "우리 아키텍처가 최고다..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "여러분들께 감사드린다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](figures/a1.png)\n",
    "![Image](figures/a2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
